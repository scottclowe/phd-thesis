@article{Banerjee2015,
archivePrefix = {arXiv},
arxivId = {1509.03706},
author = {Banerjee, Pradeep Kr. and Griffith, Virgil},
eprint = {1509.03706},
file = {:home/scott/Documents/Mendeley Desktop/2015/Banerjee, Griffith - 2015 - CoRR.pdf:pdf},
journal = {CoRR},
keywords = {common and private information,dancy,information lattice,partial information,redun-,sufficient statistic,synergy},
title = {{Synergy, Redundancy and Common Information}},
url = {http://arxiv.org/abs/1509.03706{\%}5Cnhttp://arxiv.org/pdf/1509.03706 http://arxiv.org/abs/1509.03706},
volume = {abs/1509.0},
year = {2015}
}
@article{Brenner2000,
abstract = {We show that the information carried by compound events in neural spike trains-patterns of spikes across time or across a population of cells-can be measured, independent of assumptions about what these patterns might represent. By comparing the information carried by a compound pattern with the information carried independently by its parts, we directly measure the synergy among these parts. We illustrate the use of these methods by applying them to experiments on the motion-sensitive neuron H1 of the fly's visual system, where we confirm that two spikes close together in time carry far more than twice the information carried by a single spike. We analyze the sources of this synergy and provide evidence that pairs of spikes close together in time may be especially important patterns in the code of H1.},
author = {Brenner, N and Strong, S P and Koberle, R and Bialek, W and {de Ruyter van Steveninck}, R R},
file = {:home/scott/Documents/Mendeley Desktop/2000/Brenner et al. - 2000 - Neural Computation.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {Action Potentials,Action Potentials: physiology,Afferent,Afferent: physiology,Animals,Diptera,Models,Neurological,Neurons,Pattern Recognition,Visual,Visual Pathways,Visual Pathways: physiology,Visual: physiology},
month = {jul},
number = {7},
pages = {1531--52},
pmid = {10935917},
title = {{Synergy in a neural code}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10935917},
volume = {12},
year = {2000}
}
@incollection{Griffith2014,
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {arXiv:1205.4265v3},
author = {Griffith, Virgil and Koch, Christof},
booktitle = {Guided Self-Organization: Inception},
doi = {10.1007/978-3-642-53734-9_6},
editor = {Prokopenko, Mikhail},
eprint = {arXiv:1205.4265v3},
file = {:home/scott/Documents/Mendeley Desktop/2014/Griffith, Koch - 2014 - Guided Self-Organization Inception.pdf:pdf},
isbn = {978-3-642-53734-9},
pages = {159--190},
publisher = {Springer Berlin Heidelberg},
title = {{Quantifying Synergistic Mutual Information}},
url = {http://arxiv.org/abs/1205.4265 http://dx.doi.org/10.1007/978-3-642-53734-9{\_}6},
year = {2014}
}
@article{Latham2005,
abstract = {Decoding the activity of a population of neurons is a fundamental problem in neuroscience. A key aspect of this problem is determining whether correlations in the activity, i.e., noise correlations, are important. If they are important, then the decoding problem is high dimensional: decoding algorithms must take the correlational structure in the activity into account. If they are not important, or if they play a minor role, then the decoding problem can be reduced to lower dimension and thus made more tractable. The issue of whether correlations are important has been a subject of heated debate. The debate centers around the validity of the measures used to address it. Here, we evaluate three of the most commonly used ones: synergy, $\Delta$Ishuffled, and $\Delta$I. We show that synergy and $\Delta$Ishuffled are confounded measures: they can be zero when correlations are clearly important for decoding and positive when they are not. In contrast, $\Delta$I is not confounded. It is zero only when correlations are not important for decoding and positive only when they are; that is, it is zero only when one can decode exactly as well using a decoder that ignores correlations as one can using a decoder that does not, and it is positive only when one cannot decode as well. Finally, we show that $\Delta$I has an information theoretic interpretation; it is an upper bound on the information lost when correlations are ignored.},
author = {Latham, Peter E and Nirenberg, Sheila},
doi = {10.1523/JNEUROSCI.5319-04.2005},
file = {:home/scott/Documents/Mendeley Desktop/2005/Latham, Nirenberg - 2005 - Journal of Neuroscience.pdf:pdf},
issn = {0270-6474},
journal = {Journal of Neuroscience},
keywords = {,Action Potentials,Animals,Humans,Models,Neurological,Neurons,Neurons: physiology,Statistics as Topic},
month = {may},
number = {21},
pages = {5195--5206},
pmid = {15917459},
publisher = {Society for Neuroscience},
title = {{Synergy, Redundancy, and Independence in Population Codes, Revisited}},
url = {http://www.jneurosci.org/content/25/21/5195},
volume = {25},
year = {2005}
}
@article{Magri2009,
abstract = {Information theory is an increasingly popular framework for studying how the brain encodes sensory information. Despite its widespread use for the analysis of spike trains of single neurons and of small neural populations, its application to the analysis of other types of neurophysiological signals (EEGs, LFPs, BOLD) has remained relatively limited so far. This is due to the limited-sampling bias which affects calculation of information, to the complexity of the techniques to eliminate the bias, and to the lack of publicly available fast routines for the information analysis of multi-dimensional responses.},
author = {Magri, Cesare and Whittingstall, Kevin and Singh, Vanessa and Logothetis, Nikos K and Panzeri, Stefano},
doi = {10.1186/1471-2202-10-81},
file = {:home/scott/Documents/Mendeley Desktop/2009/Magri et al. - 2009 - BMC neuroscience.pdf:pdf},
issn = {1471-2202},
journal = {BMC neuroscience},
keywords = {Algorithms,Animals,Automatic Data Processing,Brain,Brain: physiology,Computer Simulation,Computer-Assisted,Electroencephalography,Electrophysiology,Information Theory,Macaca,Models,Neurological,Neurons,Neurons: physiology,Signal Processing},
month = {jan},
number = {81},
pmid = {19607698},
title = {{A toolbox for the fast information analysis of multiple-site LFP, EEG and spike train recordings}},
volume = {10},
year = {2009}
}
@article{Montemurro2007,
abstract = {The estimation of the information carried by spike times is crucial for a quantitative understanding of brain function, but it is difficult because of an upward bias due to limited experimental sampling. We present new progress, based on two basic insights, on reducing the bias problem. First, we show that by means of a careful application of data-shuffling techniques, it is possible to cancel almost entirely the bias of the noise entropy, the most biased part of information. This procedure provides a new information estimator that is much less biased than the standard direct one and has similar variance. Second, we use a nonparametric test to determine whether all the information encoded by the spike train can be decoded assuming a low-dimensional response model. If this is the case, the complexity of response space can be fully captured by a small number of easily sampled parameters. Combining these two different procedures, we obtain a new class of precise estimators of information quantities, which can provide data-robust upper and lower bounds to the mutual information. These bounds are tight even when the number of trials per stimulus available is one order of magnitude smaller than the number of possible responses. The effectiveness and the usefulness of the methods are tested through applications to simulated data and recordings from somatosensory cortex. This application shows that even in the presence of strong correlations, our methods constrain precisely the amount of information encoded by real spike trains recorded in vivo.},
author = {Montemurro, M A and Senatore, R and Panzeri, S},
doi = {10.1162/neco.2007.19.11.2913},
file = {:home/scott/Documents/Mendeley Desktop/2007/Montemurro, Senatore, Panzeri - 2007 - Neural computation.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Automatic Data Processing,Bias (Epidemiology),Brain,Brain: cytology,Brain: physiology,Computer Simulation,Humans,Information Theory,Markov Chains,Models,Neurological,Neurons,Neurons: physiology},
month = {nov},
number = {11},
pages = {2913--57},
pmid = {17883346},
title = {{Tight data-robust bounds to mutual information combining shuffling and model selection techniques}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17883346},
volume = {19},
year = {2007}
}
@article{Nemenman2004,
abstract = {The major problem in information theoretic analysis of neural responses and other biological data is the reliable estimation of entropy-like quantities from small samples. We apply a recently introduced Bayesian entropy estimator to synthetic data inspired by experiments, and to real experimental spike trains. The estimator performs admirably even very deep in the undersampled regime, where other techniques fail. This opens new possibilities for the information theoretic analysis of experiments, and may be of general interest as an example of learning from limited data.},
author = {Nemenman, Ilya and Bialek, William and {de Ruyter van Steveninck}, Rob},
file = {:home/scott/Documents/Mendeley Desktop/2004/Nemenman, Bialek, de Ruyter van Steveninck - 2004 - Physical review. E, Statistical, nonlinear, and soft matter physics.pdf:pdf},
issn = {1539-3755},
journal = {Physical review. E, Statistical, nonlinear, and soft matter physics},
keywords = {bayesian statistics,entropy,estimation,information,neural code,neuroscience},
month = {may},
number = {5 Pt 2},
pages = {056111},
pmid = {15244887},
title = {{Entropy and information in neural spike trains: progress on the sampling problem.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15244887},
volume = {69},
year = {2004}
}
@article{Panzeri2001,
author = {Panzeri, Stefano and Schultz, SR},
file = {:home/scott/Documents/Mendeley Desktop/2001/Panzeri, Schultz - 2001 - Neural Computation.pdf:pdf},
journal = {Neural Computation},
pages = {1311--1349},
title = {{A unified approach to the study of temporal, correlational, and rate coding}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/08997660152002870},
volume = {13},
year = {2001}
}
@article{Panzeri2007,
abstract = {Information Theory enables the quantification of how much information a neuronal response carries about external stimuli and is hence a natural analytic framework for studying neural coding. The main difficulty in its practical application to spike train analysis is that estimates of neuronal information from experimental data are prone to a systematic error (called "bias"). This bias is an inevitable consequence of the limited number of stimulus-response samples that it is possible to record in a real experiment. In this paper, we first explain the origin and the implications of the bias problem in spike train analysis. We then review and evaluate some recent general-purpose methods to correct for sampling bias: the Panzeri-Treves, Quadratic Extrapolation, Best Universal Bound, Nemenman-Shafee-Bialek procedures, and a recently proposed shuffling bias reduction procedure. Finally, we make practical recommendations for the accurate computation of information from spike trains. Our main recommendation is to estimate information using the shuffling bias reduction procedure in combination with one of the other four general purpose bias reduction procedures mentioned in the preceding text. This provides information estimates with acceptable variance and which are unbiased even when the number of trials per stimulus is as small as the number of possible discrete neuronal responses.},
author = {Panzeri, Stefano and Senatore, Riccardo and Montemurro, Marcelo A and Petersen, Rasmus S},
doi = {10.1152/jn.00559.2007},
file = {:home/scott/Documents/Mendeley Desktop/2007/Panzeri et al. - 2007 - Journal of Neurophysiology.pdf:pdf},
issn = {0022-3077},
journal = {Journal of Neurophysiology},
keywords = {Animals,Biometry,Entropy,Humans,Models,Neurological,Neurons,Neurons: physiology,Neurophysiology,Neurophysiology: statistics {\&} numerical data,Probability,Selection Bias},
month = {sep},
number = {3},
pages = {1064--72},
pmid = {17615128},
title = {{Correcting for the sampling bias problem in spike train information measures}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17615128},
volume = {98},
year = {2007}
}
@article{Panzeri1996,
author = {Panzeri, Stefano and Treves, Alessandro},
file = {:home/scott/Documents/Mendeley Desktop/1996/Panzeri, Treves - 1996 - Network Computation in Neural Systems.pdf:pdf},
journal = {Network: Computation in Neural Systems},
pages = {87--107},
title = {{Analytical estimates of limited sampling biases in different information measures}},
volume = {7},
year = {1996}
}
@article{Pola2003,
abstract = {We derive a new method to quantify the impact of correlated firing on the information transmitted by neuronal populations. This new method considers, in an exact way, the effects of high order spike train statistics, with no approximation involved, and it generalizes our previous work that was valid for short time windows and small populations. The new technique permits one to quantify the information transmitted if each cell were to convey fully independent information separately from the information available in the presence of synergy-redundancy effects. Synergy-redundancy effects are shown to arise from three possible contributions: a redundant contribution due to similarities in the mean response profiles of different cells; a synergistic stimulus-dependent correlational contribution quantifying the information content of changes of correlations with stimulus, and a stimulus-independent correlational contribution term that reflects interactions between the distribution of rates of individual cells and the average level of cross-correlation. We apply the new method to simultaneously recorded data from somatosensory and visual cortices. We demonstrate that it constitutes a reliable tool to determine the role of cross-correlated activity in stimulus coding even when high firing rate data (such as multi-unit recordings) are considered.},
annote = {From Duplicate 1 ( An exact method to quantify the information transmitted by different mechanisms of correlational coding - Pola, G; Thiele, A; Hoffmann, K P; Panzeri, S )
},
author = {Pola, G and Thiele, A and Hoffmann, K P and Panzeri, S},
file = {:home/scott/Documents/Mendeley Desktop/2003/Pola et al. - 2003 - Network (Bristol, England).pdf:pdf},
issn = {0954-898X},
journal = {Network (Bristol, England)},
keywords = {Action Potentials,Computer Simulation,Information Theory,Models,Neural Conduction,Neural Networks (Computer),Neurological,Neurons,Neurons: physiology,Reaction Time,Reproducibility of Results,Somatosensory Cortex,Somatosensory Cortex: physiology,Statistics as Topic,Statistics as Topic: methods,Time Factors},
month = {feb},
number = {1},
pages = {35--60},
pmid = {12613551},
title = {{An exact method to quantify the information transmitted by different mechanisms of correlational coding}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12613551},
volume = {14},
year = {2003}
}
@article{Schneidman2003,
author = {Schneidman, Elad and Bialek, William and {Berry II}, Michael J},
doi = {10.1523/JNEUROSCI.5319-04.2005},
file = {:home/scott/Documents/Mendeley Desktop/2003/Schneidman, Bialek, Berry II - 2003 - The Journal of Neuroscience.pdf:pdf},
issn = {1529-2401},
journal = {The Journal of Neuroscience},
month = {may},
number = {37},
pages = {11539--11553},
pmid = {15917459},
title = {{Synergy, redundancy, and independence in population codes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15917459},
volume = {23},
year = {2003}
}
@article{Shannon1948,
author = {Shannon, C. E.},
file = {:home/scott/Documents/Mendeley Desktop/1948/Shannon - 1948 - The Bell System Technical Journal.pdf:pdf},
journal = {The Bell System Technical Journal},
number = {July 1948},
pages = {379--423},
title = {{A Mathematical Theory of Communication}},
volume = {27},
year = {1948}
}
@article{Treves1995,
author = {Treves, Alessandro and Panzeri, Stefano},
doi = {10.1162/neco.1995.7.2.399},
file = {:home/scott/Documents/Mendeley Desktop/1995/Treves, Panzeri - 1995 - Neural Computation.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {mar},
number = {2},
pages = {399--407},
title = {{The Upward Bias in Measures of Information Derived from Limited Data Samples}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/neco.1995.7.2.399},
volume = {7},
year = {1995}
}
@article{Williams2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1004.2515v1},
author = {Williams, Paul L and Beer, Randall D},
eprint = {arXiv:1004.2515v1},
file = {:home/scott/Documents/Mendeley Desktop/2010/Williams, Beer - 2010 - CoRR.pdf:pdf},
journal = {CoRR},
keywords = {,information theory,interaction information,multivariate interaction,redundancy,synergy},
title = {{Nonnegative Decomposition of Multivariate Information}},
url = {http://arxiv.org/abs/1004.2515},
volume = {abs/1004.2},
year = {2010}
}
