%!TEX root = ../PhDthesis.tex
\chapter{Discussion}

In this thesis, we have ... generic


\section{Summary}

\subsection{Perceptual learning}

Together, our results show the most informative signal about the contrast of the stimulus within the cortex is contained in the initial response to the stimulus onset within \ac{V1}, and this does not rise with training.
The population activity in \ac{V4} rises with training, in line with the rise in behavioural performance of the subject.
This indicates that \ac{V4} is trained to be better at reading out the information in \ac{V1} relevant to the task.
Our results also indicate that feedback signals from higher cortical regions into both \ac{V1} and \ac{V4} become more pronounced with training.


\subsection{Laminar distribution of information}



\section{Open directions for future research}

There are obviously several directions for future work to expand on this study, some of which are more likely to bear fruit than others.


\subsection{Perceptual learning}

We identified the narrow beginning of the \ac{V1} stimulus-onset response as the most informative cortical signal conveying information about the contrast of the stimulus, and concluded this was likely to be because the latency of the signal reaching the cortex was sensitive to the contrast of the stimulus presented.
Consequently, it would be useful to investigate the amount of information encoded in the latency of the first spike in response to the stimulus onset.
That said, the spontaneous firing rate before the stimulus is around \SI{7}{Hz} (shown in \autoref{fig:fr_hm}), which implies a spontaneously generated spike will occur in the first \SI{50}{\milli\second} around \SI{35}{\percent} of the time.
With this in mind, the time of the second spike after stimulus onset may prove even more informative.

We used a Fisher linear discriminant classifier to decoded information in the population activity, and alternatives to this could be explored.
Linear models, such as linear regression or support vector machines would likely give similar performance to the Fisher linear discriminant which we employed.
Non-linear models such as a multi-layered perceptron neural network may be able to capture information in the population activity which was lost when we made the assumption of monotonic tuning curves, however the difference in effect which would result is not likely to be very large.

In our study, we trained the classifier on trials originating during an individual session, and evaluated it against the performance from held out trials from the same session.
Consequently, it is possible for the model which we construct to deviate between sessions --- if the structure of the population activity changes over time the classifier built for the final session might be quite different from the classifier trained on the data from the first session.
Allowing the model built by the classifier to change over time corresponds with the implicit assumption that the higher-cortical areas can, at will, change the mapping they employ to decode the results of lower-cortical areas.
Instead, we could consider the implications of a fixed mapping from low to high cortical regions, for instance by training a decoder on data from the initial sessions, then fixing the decoder when evaluating the amount of information present in the later sessions.
If there is little difference in performance between the two methods, this would suggest that the cortical region under consideration is directed to improve its encoding of the data by higher cortical regions, or is under the constraint of a certain decoding model employed by higher-cortical regions.

Instead of training a decoder to classify the stimulus and investigating the agreement between the output of this classifier and the behavioural response, we could train a decoder to predict the behavioural response directly.
Such a procedure would be similar to a brain-machine interface.

The decoder-based population analysis from \autoref{sec:dec-meth-lin} and \autoref{sec:pl_agreement} could also be applied to the population activity collected over shorter windows, such as the few tens of milliseconds surrounding stimulus onset response.
In doing so, we could repeat the results of \autoref{sec:pl_info_latency}, but for the information encoded in the population activity instead of the average information encoded by individual channels.
The final outcome of this would be a heatmap similar to \autoref{fig:info_offset_vs_winlen_dif} showing when the population activity becomes more or less informative over time.
However, we anticipate that the results would be similar to the ones we already have, just with a larger effect size (since the population is more informative than any individual channel) and without statistics (since we have many channels but only one neural population), and would not yield any more insight into the neural changes relating to perceptual learning.

Similarly, we could apply the population activity decoder to the activity during the stimulus-off period, as we performed in \autoref{sec:pl_poststim_info} for individual channels.
Again, we expect this would corroborate the results we have already reported.
But since the effect size for post-stimulus information about the stimulus contained in individual channels was low, it would be useful to repeat this analysis using the population activity.
This section of the analysis could also benefit from computing the conditional mutual information between the neural activity and the behavioural response, conditioned on the true stimulus group.

Similar to the procedure we later used in \autoref{sec:lam_redundancy}, we could compute the redundancy between pairs of channels for the information they encode about the stimulus, and see how the redundancy changes with training.
This would have to be reported with care, since the absolute amount of information encoded in the channels changes (typically increasing, but not always) with training.
For instance, if the information encoded in each channel increases and some of the increase is the same information for each channel, this will cause the redundancy to rise.
Consequently, it may be more interesting to consider the relative redundancy, normalised against the total information encoded in one or both of the channels, instead.


\section{Laminar information}

Firstly, multi-unit spiking activity has frequency components extending into the high-gamma range at around \SI{100}{Hz} \citep{Einevoll2013}.
In addition to this, recent work by \citet{Zanos2011} has indicated that low-frequency components of the sharp changes in voltage in the broadband signal which associated with spikes are retained in \ac{LFP} extracted from the broadband signal.
As a consequence, there spurious correlations between the \ac{LFP} and \ac{MUA}.
These spurious correlations may impact our results, and it would be prudent remove the waveforms of the spikes from the broadband signal before extracting the power of \ac{LFP} and \ac{CSD} oscillations \citep{Zanos2011} and confirm that the \SIrange{60}{170}{Hz} power is still redundant with the \ac{MUA}.

We determined the \ac{CSD} from the \ac{LFP} using the inverse \ac{CSD} method (iCSD; \citealp{Pettersen2006}).
However, the authors have since detailed a more advanced procedure for estimating the \ac{CSD} from \acp{LFP}.
This, the kernel \acl{CSD} method (kCSD; \citealp{Potworowski2012}), is non-parametric and uses Gaussian kernels with regularisation to estimate the ground truth \ac{CSD}.
In particular, kCSD provides a native handling for unevenly spaced signal samples, which is useful since we had a small number of faulty electrode contacts, leaving holes in our sampling grid.
Re-extracting the \ac{CSD} with kCSD would be more accurate, but is unlikely to perturb our results by a large amount.

When determining which spatiotemporal components of the stimulus corresponded to the changes in cortical power and phase, we focused on the rate of change of luminance.
However, we did not find information about any spatiotemporal scales present in the phase of oscillations.
Consequently, it would be prudent to widen our search and consider colour-opponent changes in the stimulus, as is provided to the visual cortex through the P-pathway and K-pathway.
One could even go so far as to model the transformations to the raw visual input performed by each of the \acp{RGC} types.
In doing so, we would simulate the full effects of processing in the retina and be able to investigate the structure of information in \ac{V1} with respect to its actual input.
However, such an undertaking would be quite significant, since our understanding the computational processing within the retina remains incomplete and is actively researched by many groups.

For this project, we principally investigated the population activity by considering the \ac{LFP} and \ac{CSD}.
However, the process through which each of the different types of neuron within \ac{V1} manifest \acp{CSD} and how each frequency component in the signal is generated is not yet well understood.
Compartmental models of the morphology of cortical neurons can be used to fill in such gaps of understanding \citep{Leski2013}.
If we were to reconstruct the morphology of each cell type within \ac{V1} and derive the \ac{CSD} generated by each, we would be much better equipped to understand which neurons generate the information-encoding oscillations which we have described and localised in this thesis.
